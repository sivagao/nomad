{"data":{"mongodbDatabaseArticles":{"id":"5b9248d587be056953db7bfe","content":"<div><p>This year I learnt how to build, run and maintain my own server. It now runs on <a href=\"//www.linode.com/?r=9ca5a3583601b91e04ba71883446f0ed3d8fd025\">Linode</a> and runs all websites for my projects including all <a href=\"/12-startups-12-months/\">12 startups in 12 months</a>. It runs <a href=\"http://nginx.org/\">nginx</a> and manages to <a href=\"/product-hunt-hacker-news-number-one/\">survive 1,000+ users per second from Hacker News and Product Hunt</a> out-of-the-box fine. I still think that's a miracle since I'm a supernoob and I set it up myself and it's just 4GB ram Linode server. With my startup <a href=\"http://nomadlist.io/\">Nomad List</a> now getting increasingly popular, my server is now also becoming increasingly important for me now. If it dies, I'm completely f*d. So I want to start doing regular backups. Today I'm going to find out how to do that easily and cheap. And share the results with you &lt;3 Linode has <a href=\"//www.linode.com/backups\">backups</a> built-in which is awesome. It's just a little per month extra. But storing your server and backups at the same hosting company is a <a href=\"http://en.wikipedia.org/wiki/Single_point_of_failure\">single point of failure</a>. Therefore, I want to store another backup somewhere else.</p> <p>So today I'm going to make an automatic offsite backup program. It can also be used for people with Digital Ocean VPS server or any other VPS really. Mine runs Ubuntu which is kind of standard now.</p> <h2>Solution</h2> <p><img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\"></p><img src=\"/wp-content/uploads/2014/12/2014-12-16-backup-vps-to-s3.jpg\"><br> So where should we backup to? Well, Amazon has a huge server company that hosts a large part of the internet (e.g. Dropbox and major websites) called <a></a><p>So where should we backup to? Well, Amazon has a huge server company that hosts a large part of the internet (e.g. Dropbox and major websites) called <a href=\"http://aws.amazon.com/\">AWS</a> . Most of you know this. And most of you might also know its service called S3. It's a storage service many websites use to hold their static content (like images etc.). It's redundant storage which means your files are probably not going to dissappear. It also has its own protocol s3:// that a lot of desktop and Linux apps support.</p><p>What's great is that transferring to S3 is literally <a href=\"http://aws.amazon.com/s3/pricing/\">free</a>. And storing it is only 3 cents per GB. My VPS is about 80 GB, so that's $2.40/m for a redundant backup. Awesome, right?</p> <p>Also their data center looks really cool. Who doesn't want their files in here?</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\"><img src=\"/wp-content/uploads/2014/12/08bits-amazon-tmagArticle.jpg\"> <p>Let's use S3 to set up a weekly backup.</p> <h2>Set up S3</h2> <p>If you're not yet, let's sign up to Amazon AWS <a href=\"http://aws.amazon.com/\">here</a>.</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\" alt=\"Screenshot 2014-12-16 00.16.53\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-16-00.16.53-1024x545.png\" alt=\"Screenshot 2014-12-16 00.16.53\"> <p>Then <a href=\"//signin.aws.amazon.com/\">sign in to AWS</a> and click <a href=\"//console.aws.amazon.com/s3/home\">S3 Scalable Storage in the Cloud</a>.</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-15-23.49.08-1024x401.png\"> <p>We now want to create a bucket. A bucket is just a virtual server that stores your data.</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\" alt=\"Screenshot 2014-12-15 23.51.34\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-15-23.51.34-1024x59.png\" alt=\"Screenshot 2014-12-15 23.51.34\"> <p>You can pick a region, that's where your server will be located. You can't change that. You can create a new bucket and transfer everything to the other region but that DOES cost money. So make sure you pick the right region.</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-15-23.51.24-1024x598.png\"> <p>Give your bucket a name with ONLY alphanumeric characters. Make sure it's kinda descriptive so you remember what the bucket is for.</p> <p>My server is in London, so I picked United States as a region. Why? Well, I'm <a href=\"/backups-solar-flares-cookie-jar-faraday/\">paranoid about data</a> and what if a meteorite drops on the UK, where's my data then, right?</p> <p>After creating your S3 bucket, you need to set up your security credentials so you can let your VPS server access it. </p> <p>Now you can do this super advanced <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\">writing your own security policies</a>, but it's just too hard for me. So instead I'm going to do something not so secure which is give your server access to your ENTIRE AWS account. This is super unsafe IF you have other stuff in that account. If you only have this bucket in there, it's fine.</p> <p>Click on your username (at the top-right) and select <a href=\"//console.aws.amazon.com/iam/home#security_credential\">Security Credentials</a>.</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\" alt=\"Screenshot 2014-12-15 23.57.42\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-15-23.57.42-1024x239.png\" alt=\"Screenshot 2014-12-15 23.57.42\"> <p>Then click \"Delete your root access keys\" and select \"Manage Security Credentials\".</p> <img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-15-23.59.45-1024x325.png\" alt=\"Screenshot 2014-12-15 23.59.45\" width=\"640\" height=\"203\" class=\"alignnone size-large wp-image-4077 responsively-lazy\"> <p>From there click \"Access Keys\" and select \"Create New Access Key\".</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\" alt=\"Screenshot 2014-12-16 00.00.17\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-16-00.00.17-1024x540.png\" alt=\"Screenshot 2014-12-16 00.00.17\"> <p>Your key is now created. This key is two-part, it contains an ID code and a secret code. You need both to access your bucket.</p> <p>Click \"Show Access Key\" and save the Key and Secret code somewhere secure in a text file. You're going to need this next.</p> <img class=\"sl_lazyimg\" src=\"/wp-content/plugins/simple-lazyload/blank_250x250.gif\"><img src=\"/wp-content/uploads/2014/12/Screenshot-2014-12-16-00.00.54-1024x320.png\"> <h2>Set up S3 on your VPS</h2> <p>Now it's time to set up S3 on your VPS server.</p> <p>I picked a few things from this tutorial by <a href=\"//kura.io/2012/02/29/backup-a-linux-server-to-amazon-s3-on-debian-6ubuntu-10-04/\" class=\"broken_link\">Kura</a>. First we need to install s3cmd which is a Linux app that lets you transfer in and out of S3 super easily.</p> <p>This code install s3cmd to your server and starts it configuration.</p> <pre>&#13;\nsudo wget -O- -q http://s3tools.org/repo/deb-all/stable/s3tools.key | sudo apt-key add -&#13;\nsudo wget http://s3tools.org/repo/deb-all/stable/s3tools.list -O /etc/apt/sources.list.d/s3tools.list&#13;\nsudo apt-get update &amp;&amp; apt-get install s3cmd&#13;\nsudo s3cmd --configure&#13;\n</pre> <p>The questions in the configuration are pretty straightforward.</p> <p>Enter your access key and secret key from your security credentials that you saved before.</p> <p>At \"Encryption password:\", it's probably a good idea to specify a password with which your data will be encrypted when it transfers. You can generate a random one at <a href=\"http://www.random.org/strings/?num=10&amp;len=16&amp;loweralpha=on&amp;unique=on&amp;format=html&amp;rnd=new\">random.org</a>.</p> <p>Just press ENTER at \"Path to GPG program\", as you probably have GPG (an encryption program) installed already on your server at the default path. Press ENTER at \"Use HTTPS protocol\", since it slows it down and you're already encrypting it with GPG. I guess if you have super secure data turn this on instead. Press ENTER at \"HTTP Proxy server name\", you probably don't need that.</p> <p>Then let the configuration app test the access and if it works continue. If not, check if your S3 credentials are set correctly.</p> <pre>&#13;\nEnter new values or accept defaults in brackets with Enter.&#13;\nRefer to user manual for detailed description of all options.&#13;\n &#13;\nAccess key and Secret key are your identifiers for Amazon S3&#13;\nAccess Key: &#13;\nSecret Key: &#13;\n &#13;\nEncryption password is used to protect your files from reading&#13;\nby unauthorized persons while in transfer to S3&#13;\nEncryption password: &#13;\nPath to GPG program [/usr/bin/gpg]: &#13;\n &#13;\nWhen using secure HTTPS protocol all communication with Amazon S3&#13;\nservers is protected from 3rd party eavesdropping. This method is&#13;\nslower than plain HTTP and cannot be used if you are behind a proxy&#13;\nUse HTTPS protocol [No]: &#13;\n &#13;\nOn some networks all internet access must go through a HTTP proxy.&#13;\nTry setting it here if you cannot conect to S3 directly&#13;\nHTTP Proxy server name: &#13;\n &#13;\nNew settings:&#13;\n  Access Key: &#13;\n  Secret Key: &#13;\n  Encryption password: &#13;\n  Path to GPG program: /usr/bin/gpg&#13;\n  Use HTTPS protocol: False&#13;\n  HTTP Proxy server name: &#13;\n  HTTP Proxy server port: 0&#13;\n&#13;\nTest access with supplied credentials? [Y/n] &#13;\n</pre> <p>Now you need to connect to the bucket you created on S3:</p> <pre>&#13;\ns3cmd mb s3://nameofyours3bucket&#13;\n</pre> <p>If this works. Yay!</p> <p>The s3cmd has a command called sync which syncs TO the remote server (not back locally, so don't worry). Here's an example (don't run it!):</p> <pre>&#13;\ns3cmd sync --recursive --preserve /your-folder-name-to-backup s3://nameofyours3bucket&#13;\n</pre> <p>I've added --recursive to save all child directories and important --preserve which keeps all file attributes and permissions the same. VERY important if you'll be transferring your backups back when the shit hits the fan.</p> <p>You can now make a shell script (ending with .sh) that transfers the most important files of your server to S3 regularly. I thought about just doing </p> <pre>&#13;\ns3cmd sync --recursive --preserve /&#13;\n</pre> <p>but that also includes the ENTIRE Linux installation, I don't know if that's preferable as I only need the files that are important. The <a href=\"http://superuser.com/questions/143557/which-are-the-most-important-directories-to-backup-on-a-linux-server\">most important directories</a> seemed to be /srv (that's where I put my http files), /etc, /home and /var. </p> <p>Also it's useful to save a list of which apps you have installed on your server. You can do that with \"dpkg --get-selections\". Then save that to \"dpkg.list\" and send that to S3 too.</p> <p>The date functions logs the time so you have a better log of what's happening later:</p> <pre>&#13;\n#!/bin/sh&#13;\necho 'Started'&#13;\ndate +'%a %b %e %H:%M:%S %Z %Y'&#13;\ns3cmd sync --recursive --preserve /srv s3://nameofyours3bucket&#13;\ns3cmd sync --recursive --preserve /etc s3://nameofyours3bucket&#13;\ns3cmd sync --recursive --preserve /home s3://nameofyours3bucket&#13;\ns3cmd sync --recursive --preserve /var s3://nameofyours3bucket&#13;\ndpkg --get-selections &gt; dpkg.list&#13;\ns3cmd sync --recursive --preserve dpkg.list s3://nameofyours3bucket&#13;\ndate +'%a %b %e %H:%M:%S %Z %Y'&#13;\necho 'Finished'&#13;\n</pre> <p>Save this file as backupToS3.sh. I've saved it in /srv on my server.<br> Test it by running it first and typing this in your shell:</p> <pre>&#13;\nsh backupToS3.sh&#13;\n</pre> <p>If it transfers well you can make it in to a regularly scheduled job by setting up a cron job by typing this in your shell:</p> <pre>&#13;\nsudo crontab -e&#13;\n</pre> <p>Then you'll see a text editor with all your scheduled cron jobs. Add this line at the top:</p> <pre>&#13;\n@weekly /srv/backupToS3.sh &gt; /srv/backupToS3.txt&#13;\n</pre> <p>This will run the backup weekly and save the output to a .txt log file that you can check to see if it ran correctly.</p> <p>You can also use @daily and @monthly on most Linux installations. But if the backup takes a long time it might take longer than a day. So it'll start while the other one is still running. Eek.</p> <p>So that's it! Mini-tip: s3cmd also runs on OSX, that means you can even backup your Mac to S3.</p> <h2>Conclusion</h2> <p>With an original copy on my server, a few Linode backups and now a redundant (!) S3 backup at Amazon, I feel a lot safer. Linode's restore backup function is super smooth, but having an EXTRA backup present when THAT restore might go wrong is great.</p> <p>Stay safe! <strong>Backup ALL THE THINGS!</strong></p> <img src=\"/wp-content/uploads/2014/12/78ba26a9810c0f04beb4a8d68129f81e9f3e623170960dcc1b27813c2bda7a7f.jpg\" alt=\"78ba26a9810c0f04beb4a8d68129f81e9f3e623170960dcc1b27813c2bda7a7f\" width=\"320\" height=\"240\" class=\"alignnone size-full wp-image-4073 responsively-lazy\">  <p>P.S. I just wrote a book on bootstrapping indie startups called <a href=\"//makebook.io/\">MAKE</a>. And I'm now on <a href=\"//instagram.com/levelsio\">Instagram</a> and <a href=\"//twitter.com/levelsio\">Twitter</a> too if you'd like to follow more of my adventures. I don't use email so <a href=\"//twitter.com/levelsio\">tweet me</a> your questions. If you like what I'm doing, consider backing me on <a href=\"//www.patreon.com/levelsio\">Patreon</a>. </p><p></p>  </div>","title":"How to backup your Linode or Digital Ocean VPS to Amazon S3"}},"pageContext":{"id":"5b9248d587be056953db7bfe"}}